# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vr2Srk3alCOIldJ-gNH2cFrfXuELd9EM
"""

!pip install llama_index openai selenium

import os
os.environ["OPEN_API_KEY"] = "sk-proj-U0e7dk456mTdRiCOFqlYT3BlbkFJb4KM5AzYLCIRHo9JiCs7"

from llama_index.llms import openai
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from IPython.display import Markdown, display

import os
import subprocess

# List of URLs and new filenames
pdf_files = [
    {"url": "https://arxiv.org/pdf/1706.03762.pdf", "filename": "attention-is-all-you-need.pdf"},
    {"url": "https://arxiv.org/pdf/1807.03819.pdf", "filename": "bert-pretraining-of-deep-bidirectional-transformers.pdf"},
    {"url": "https://arxiv.org/pdf/1810.04805.pdf", "filename": "gpt2-language-models-are-unsupervised-multitask-learners.pdf"},
    {"url": "https://arxiv.org/pdf/2005.14165.pdf", "filename": "t5-exploring-the-limits-of-transfer-learning.pdf"},
    {"url": "https://arxiv.org/pdf/1910.13461.pdf", "filename": "albert-a-lite-bert-for-self-supervised-learning-of-language-representations.pdf"},
    {"url": "https://arxiv.org/pdf/2003.10555.pdf", "filename": "electra-pretraining-text-encoders-as-discriminators.pdf"},
    {"url": "https://arxiv.org/pdf/2010.11929.pdf", "filename": "bart-denoising-sequence-to-sequence-pretraining-for-natural-language-generation.pdf"},
    {"url": "https://arxiv.org/pdf/1911.12237.pdf", "filename": "roberta-robustly-optimized-bert-approach.pdf"},
    {"url": "https://arxiv.org/pdf/2004.14969.pdf", "filename": "pegasus-pretraining-text-to-text-transformers-for-language-generation.pdf"},
    {"url": "https://arxiv.org/pdf/1909.11942.pdf", "filename": "xlmroberta-cross-lingual-language-model-pretraining.pdf"}
]

# Specify the directory where you want to save the PDF files
download_dir = "data"

# Create the directory if it doesn't exist
if not os.path.exists(download_dir):
    os.makedirs(download_dir)

# Iterate over the list of PDFs and download each one
for pdf in pdf_files:
    pdf_url = pdf["url"]
    new_filename = pdf["filename"]

    # Use wget to download the PDF file into the directory
    command = f"wget -P {download_dir} {pdf_url}"

    # Execute the wget command using subprocess
    try:
        subprocess.run(command, shell=True, check=True)
        print(f"PDF downloaded successfully to {download_dir}")

        # Rename the downloaded file to the desired name
        old_filepath = os.path.join(download_dir, os.path.basename(pdf_url))
        new_filepath = os.path.join(download_dir, new_filename)
        os.rename(old_filepath, new_filepath)
        print(f"PDF renamed to {new_filename}")
    except subprocess.CalledProcessError as e:
        print(f"Error downloading {pdf_url}: {e}")
    except FileNotFoundError as e:
        print(f"Error renaming {old_filepath} to {new_filepath}: {e}")

!pip install pypdf

documents = SimpleDirectoryReader("data").load_data()
# the pdf gets split into chunks called documents
print(len(documents))
print(documents[0])

import openai
openai.api_key = 'sk-proj-U0e7dk456mTdRiCOFqlYT3BlbkFJb4KM5AzYLCIRHo9JiCs7'
# Build the vector store where each document is an embedding
index = VectorStoreIndex.from_documents(documents)
print("Created vector store")

query_engine = index.as_query_engine()
query_results = query_engine.query("What is attention and how is it relevant to AI and who are some notable scientists in this field?")

# Assuming query_results has an attribute 'response' containing the text:
context = query_results.response
print(context)

query = "What is attention and how is it relevant to AI and who are some notable scientists in this field?"
response = openai.chat.completions.create(
    model="gpt-4",  # Use the appropriate model
    messages=[
        {"role": "system", "content": "You are a helpful assistant and you will provide the answers in bullet point form"},
        {"role": "user", "content": f"Based on the provided context, answer the query:\n\nContext:\n{context}\n\nQuery:\n{query}"}
    ],
    max_tokens=500,  # Adjust max tokens to get longer responses
    temperature=0.7,  # Adjust temperature to control creativity
    top_p=0.9  # Adjust top_p to control diversity
)
answer = response.choices[0].message.content.strip()
print("Answer:")
print(answer)

index.storage_context.persist()
